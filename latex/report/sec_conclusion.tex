% ---------------------------------------------------------------------------- %
% CONCLUSION
% ---------------------------------------------------------------------------- %
\section{Conclusion} \label{sec:conclusion}
Following up on the work of \cite{Arge96}, we have expanded the use of
time-forward processing to create an I/O efficient \Reduce\ and \Apply\
algorithm to propose I/O efficient algorithms for all the most commonly used
OBDD algorithms. Since none of these algorithms use the block size $B$ or the
memory size $M$, then the I/O optimal sorting algorithm and priority queue can
be replaced for a Cache-oblivious or Cache-aware implementation to immediately
yield an optimal algorithm of both types. The I/O and time complexity of these
algorithms are summarised in Table~\ref{tab:summary_efficiency}.

\begin{table}[ht!]
  \centering
  \begin{tabular}{c | c | c}
    \emph{Algorithm} & \emph{I/O complexity} & \emph{Time complexity}
    \\ \hline
    \Evaluate & $O(\min(n, N/B))$ & $O(n)$
    \\
    \Reduce & $O(\sort(N))$ & $O(\sort(N))$
    \\
    \Restrict & $O(\sort(N))$ & $O(\sort(N))$
    \\
    \Apply & $O(\sort(N_1 N_2))$ & $O(\sort(N_1 N_2))$
    \\
    \Equal & $O(\sort(\min(N_1, N_2)))$ & $O(\sort(\min(N_1, N_2)))$
    \\
  \end{tabular}
  \caption{The worst-case I/O and time complexity of the proposed algorithms.
    $N$ is the number of nodes in an OBDD while $n$ is the number of variables}
  \label{tab:summary_efficiency}
\end{table}

We do not prove any I/O lower bound for these algorithms, leaving the I/O
optimality of all algorithms but of \Reduce\ (proven in \cite{Arge96}) still an
open problem.

All proposed algorithms make use of sorting nodes by multiple values or on the
values of their children. We optimise this by providing a layout of the data of
each node and its outgoing arcs, such that these sortings reduce to the much
faster sorting of $64$-bit integers. Furthermore, since all algorithms that
transform the OBDDs make use of a following \Reduce\ we also have looked into
optimisations for the algorithm as described in \cite{Arge96}. We propose to
handle recursion on sinks outside the priority queue and to apply the first
reduction rule of \cite{Bryant86} early to minimise the number of nodes sorted
for the second reduction rule. This reduces the amount of data, that is part of
the bottleneck of the algorithm.

% If promising, then future work:
% - Currently all nodes are in reverse order of how they are processed
%   - If these algorithm could output in the order of the next, then this could
%     lead to a speedup by pipelining the execution of all these algorithms!
% - Complement edges to minimise the size of the OBBD

These algorithms make use of multiple $O(N)$ sized lists and priority queues,
which may result in a linear increase of the space used compared to other
algorithms. Furthermore, the algorithms \Restrict\ and \Apply\ make use of a
subsequent separate \Reduce\ algorithm. For the \Apply\ algorithm, this may
result in a massive increase of the intermediate result to be reduced.  We
leave for a later practical implementation the question of whether this increase
in space usage and separation of algorithms to guarantee I/O efficiency is worth
it.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
