% ---------------------------------------------------------------------------- %
% THE BASIC ALGORITHM DESCRIPTION
% ---------------------------------------------------------------------------- %
\newpage
\section{OBDD manipulations using Time-Forward Processing} \label{sec:theory}

For completeness we provide both the \Reduce\ and \Apply\ algorithms of
\cite{Arge96}. We expand on these algorithms with a set of other algorithms for
the manipulation of OBDD.

The underlying idea of all the algorithms given below is to exploit that OBDD's
are directed acyclic graphs. If all nodes of an OBDD is sorted first by their
label and secondly by their unique identifier, then that constitutes a
topological sorting. As long as the dependency of computation only is one-way in
the OBDD, then a recursive algorithm can instead be handled by a single sweep
through all nodes in topological order and ``forward'' information to its
children or parent. This \emph{Time-Forward Processing} is done by use of one
more priority queues that are carefully synchronised with the scanning of the
nodes \todocite.

These queues are aligned with the scan through the list of nodes by sorting all
their entries by the same topological ordering. At the time of forwarding the
information, the receiving node is most likely not in memory. Hence, a lookup of
the receiving node would result in a full I/O used, which is not desired. To
mitigate this, all nodes store both the label and unique identifier of their
children. Since the label for the children is stored within the parent, then
there is no need to explicitly store the $0$- and $1$-sinks as nodes in the
graph. The data stored for each node then is as depicted in
Code~\ref{lst:data_node}.
\begin{lstfloat}[ht!]
  \centering

  \input{../listing/data_node.tex}

  \caption{The information stored in each node of the OBDD}
  \label{lst:data_node}
\end{lstfloat}

The reader should notice, that the Nodes contains no direct pointers to its
children, but instead only an index of where to find it in the array of all
nodes. This is a direct consequence of the iterative design of all the
algorithms below. All manipulating algorithms are followed by a \Reduce, which
outputs the nodes bottom-up. That means, that an OBDD as outputted by \Reduce\
can be traversed with the \Evaluate\ function in Code~\ref{lst:evaluate}, which
easily can be seen for $n$ variables will take $O(n)$ time and $O(\min(n, N/B))$
I/Os.
\begin{lstfloat}[ht!]
  \centering

  \input{../listing/evaluate_algorithm.tex}

  \caption{The \Evaluate\ algorithm to traverse a reduced OBDD $G$ according to an
    assignment $x$}
  \label{lst:evaluate}
\end{lstfloat}

Except \Reduce, all algorithms given below assume that the input OBDD is reduced
and in reverse topological order, as outputted by the \Reduce\ algorithm. On the
other hand, the \Reduce\ algorithm expects the to-be-reduced list of nodes are
in topological order - as outputted by the other algorithms. Due to this, all
initial sorting of the input nodes can be omitted. \cite{Arge96}

Arge~\cite{Arge96} suggests to exploit the above by rather representing the OBDD
by its edges, rather than its nodes. Since \lstinline{NodeArc} contains all the
data of a \lstinline{Node}, then this leads to a representation as a list of
\lstinline{Edge}s as in Code~\ref{lst:data_edge}. The \Evaluate\ algorithm above
still works by finding the out-going arcs of node $i$ at indices $2i$ and
$2i+1$. In fact, all node-based algorithms below may as well be done on the
edge-based representation.

\begin{lstfloat}[ht!]
  \centering

  \input{../listing/data_edge.tex}

  \caption{The information stored in each edge of the edge-based representation}
  \label{lst:data_edge}
\end{lstfloat}

One should note, that a single node now takes up at least $4/3$ as much space as
before, since the \lstinline{source} has to be stored for both edges. In fact
nothing is gained when the edges are in topological order. On the other hand,
reverse topological order is merely sorting the list of edges by
\lstinline{target}. From this, we notice that one of the supporting data
structures of the \Reduce\ algorithm as described by Arge~\cite{Arge96} is
exactly the edge-based representation sorted by \lstinline{target}. Hence, we
will opt to merge this data structure and the input OBDD. This both simplifies
the algorithm, but much more importantly it also reduces the memory footprint by
at least a third.

\subsection{Reduce} \label{sec:theory__reduce}
The \Reduce\ algorithm makes use of the following two data structures:

\begin{itemize}
\item The input list \ReduceLforward\ of \lstinline{Edge}s sorted in decreasing
  order with respect to \lstinline{target}. This provides a work-order to
  processing the nodes in reverse topological order for the reduction.
  Furthermore, this also directly provides which nodes \lstinline{s} depend on
  the computation result \lstinline{t'} of a node \lstinline{t}.

\item The priority queue \ReduceQdep\ of \lstinline{Edge}s sorted with
  respect to \lstinline{source} and secondly by \lstinline{is_high}.

  This priority queue is used to forward the result \lstinline{t'} of processing
  nodes \lstinline{t} as per \ReduceLforward\ to any nodes \lstinline{s} with an
  edge to \lstinline{t}.
\end{itemize}
We assume, that \ReduceLforward\ does not contain any edges with \lstinline{target}
a sink, but rather that \ReduceQdep\ has already been populated with all sink
dependencies. One can sort \ReduceLforward\ and move all sink edges to
\ReduceQdep\ within the $O(\sort(N))$ I/Os bound, but we will instead ensure all
algorithms in the next sections instead create \ReduceLforward\ in sorted order
and populate \ReduceQdep\ with all the sink dependencies.

The \Reduce\ algorithm applies the reduction rules of \textcite[Definition
5]{Bryant86} on all nodes of each layer using the \lstinline{reduce_layer}
subroutine depicted in Code~\ref{lst:reduce_bryant}. The output are the three
lists \lstinline{O}$_{\mathit{out}}$, \lstinline{O}$_{\mathit{red:}1}$,
\lstinline{O}$_{\mathit{red:}2}$ of tuples \lstinline{(w,w',i)} of mapping
between the original nodes \lstinline{w} of the input and nodes \lstinline{w'}
that are to be or already have been outputted. The \lstinline{i} value contains
the index of the original node from which \lstinline{w'} was derived. This then
constitutes a link between all original nodes associated by Bryant's second
reduction rule.

\begin{lstfloat}[ht!]
  \centering

  \input{../listing/reduce_layer.tex}

  \caption{Subroutine applying reduction rules of \cite{Bryant86} within \Reduce}
  \label{lst:reduce_bryant}
\end{lstfloat}

We are now ready to present the \Reduce\ algorithm as it is depicted in
Code~\ref{lst:reduce_algorithm}. The algorithm takes as argument both
datastructures described above and assumes they are initialised. It proceeds by
computing and outputting the reduced OBDD bottom-up and layer for layer
according to the reverse topological order. Upon having processed layer $j$
according to Code~\ref{lst:reduce_bryant} above, then the reduced layer is
immediately outputted and all nodes \lstinline{s} that depend on the outputted
node \lstinline{t'} in layer $j$ is forwarded \lstinline{t'} through
\ReduceQdep\ as per the edge \lstinline{(s,t)} in \ReduceLforward. Since
\ReduceQdep\ already contains all sink-dependencies and due to the bottom-up
processing of the layers then \ReduceQdep\ will always contain all dependencies
when one begins to process any layer $k < j$.

A total of $2 N$ edges will be inserted into \ReduceQdep\ and then later again
extracted, while \ReduceLforward\ is scanned once at the end to forward
information. This totals a $O(\sort(4N) + N/B) = O(\sort(N))$ number of I/Os
spent. On each layer all nodes are sorted twice, which when all layers are
combined results in another $O(\sort(N))$ total number of I/Os. Hence, the
algorithm runs within the optimal $O(\sort(N))$ I/O bound \cite{Arge96}. By
similar obsevations we can also conclude, that the algorithm has an $O(N \log_2
N)$ time complexity.

\begin{lstfloat}
  \centering

  \input{../listing/reduce_algorithm.tex}

  \caption{The \Reduce\ algorithm}
  \label{lst:reduce_algorithm}
\end{lstfloat}

\clearpage
\subsection{Restrict} \label{sec:theory__restrict}
Before we present the \Apply\ algorithm we present the simpler top-down
algorithm \Restrict, which given an OBDD $G$ and an assignment vector $A$ of
tuples $(x_i, \mathit{value})$ outputs a reduced OBDD $G_{| x_i = v_i ,\ \forall
  (x_i,v_i) \in A}$ as shown in Code~\ref{lst:restrict}.  This uses the data
structure:

\begin{itemize}
\item The priority queue \RestrictQrec\ of 3-tuples
    \lstinline{(s: Node, t: NodeArc.Link, b: bool)} sorted with respect to
    \lstinline{t} only.

  These $(s,t,b)$-tuples in the priority queue are used to forward recursion
  requests from $s$ to non-sink children $t$ along a $b$-arc.
\end{itemize}

The algorithm as depicted in Code~\ref{lst:restrict} essentially traverses the
OBDD recursively, either keeping the node and recursing along both outgoing arcs
or skipping the node and only recursing on the arc according to the assignment.
The recursion is controlled using \RestrictQrec. Since this only traverses the
data structure top-down, the nodes traversed do not know the index of their
children before they are outputted later. Hence, these nodes will not contain an
actual index to their children. Luckily, these references can be fixed as part
of the bottom-up traversal of the final \Reduce. Furthermore, due to the
top-down traversal of the OBDD, the \ReduceLforward\, \ReduceLdep\ of \Reduce\ can
be initialised during the traversal.

\begin{lstfloat}
  \centering

  \input{../listing/restrict_algorithm.tex}

  \caption{The \Restrict\ algorithm}
  \label{lst:restrict}
\end{lstfloat}

By the ordering of tuples in \RestrictQrec, nodes are visited in topological
order, which also is the blocking of \GV on disk. This means every block is
fetched at most once and at most $O(N/B)$ I/Os are used as part of the look up
of vertices in \GV. At most $2N$ requests are placed and retrieved from \Q\
which will only require $O(sort(N))$ I/Os. let $N' \leq N$ be the number of
nodes outputted for the later \Reduce, which then also only uses another
$O(sort(N')) = O(sort(N))$ I/Os. In total the \Restrict\ algorithm performs
the full substitution and reduction in $O(sort(N))$ I/Os and $O(N \log N)$ time.

\clearpage
\subsection{Apply} \label{sec:theory__apply}
Similar to \Restrict, the \Apply\ algorithm makes use of a priority queue to
forward the recursion to nodes visited later in topological order. In
\cite{Arge96} four priority queues were used to make the algorithm easier to
follow. We hope to make the algorithm still comprehensible while only using a
single queue. This will result in a constant improvement in the running time.
Since this queue has to be synchronised with the scan of both list of nodes at
the same time, then the sorting is a bit more complicated than in the prior
given algorithms. The data structure used is:
\begin{itemize}
\item The priority queue \ApplyQrec\ of 4-tuples \lstinline{(s', (t$_1$,t$_2$), b, data)}
  where \lstinline{s'} is of type \lstinline{Node}, \lstinline{t$_1$, t$_2$} of
  type \lstinline{NodeArc}, \lstinline{b} of \lstinline{bool} and
  \lstinline{data} of \lstinline{Option(Node)}.

  This forwards requests for recursion from processing the pair of nodes $(s_1,
  s_2)$ output as \lstinline{s'} to $(t_1,t_2)$ through a $b$-arc. The
  \lstinline{data} element is used when $t_1$ and $t_2$ have the same label such
  that the first visited can be forwarded to to the second for processing.
  Hence, the sorting of an element is dependant on whether \lstinline{data} is
  provided in the tuple.

  \begin{itemize}
  \item If \lstinline{data} is \lstinline{None}, then it is sorted first with
    respect to $\min(t_1.\mathit{label},t_2.\mathit{label})$, secondly with
    respect to $\min(t_1.\mathit{index},t_2.\mathit{index})$. Sinks come after
    nodes.

  \item If \lstinline{data} is \lstinline{Some(v)}, then it is still first
    sorted with respect to $\min(t_1.\mathit{label},t_2.\mathit{label})$, but
    secondly dually with respect to $\max(t_1.\mathit{index},
    t_2.\mathit{index})$. For two requests to the same $(t_1,t_2)$ the one with
    \lstinline{data} comes first.
  \end{itemize}
\end{itemize}
The reader should notice that in the general case of \lstinline{data} being
\lstinline{None}, then the sorting is a generalisation of the sorting of nodes
used up until this point.

To match the general sorting above the \lstinline{apply_step} function below in
Code~\ref{lst:apply_step} mutates variables $\mathit{iW}_1$ and $\mathit{iW}_2$
being indices in the respective input lists $G_1.V$ and $G_2.V$ of vertices
together with the matching nodes $v_1$ and $v_2$. This function used later in
\Apply, where the mutated variables are initialised.
\begin{lstfloat}
  \centering

  \input{../listing/apply_step.tex}

  \caption{Determining in which OBDD should be progressed matching the sorting}
  \label{lst:apply_step}
\end{lstfloat}

The \Apply\ algorithm constructs an OBDD that simulates both OBDDs $G_1$ and
$G_2$ at the same time. At a node representing a $(v_1,v_2)$ the label of the
output node is $\min(v_1.\mathit{label},v_2.\mathit{label})$. In the case the
two labels of $v_1$ and $v_2$ differ, then the recursion is only dependent on
the node with the smallest label. Otherwise, \lstinline{data} is used to send
the data of the first-seen node to the second for processing later, such that
both are in memory simultaneously. That is, for all cases but when
\lstinline{data} should be forwarded, then whereto recurse can be resolved as
depicted in Code~\ref{lst:apply_node}.

\begin{lstfloat}[ht!]
  \centering

  \input{../listing/apply_node.tex}

  \caption{Resolving which direction to recurse on request for $(w_1, w_2)$ while
    being at $(v_1, v_2)$.}
  \label{lst:apply_node}
\end{lstfloat}

Similar to \Restrict, this makes the algorithm traverse both OBDDs top-down
and hence output nodes before their children. This leaves no ability to include
a direct reference to them. Again, this is fixed during the later \Reduce, for
which the sorting is satisfied by merely supplying indices in order of
outputting the vertices.

The base case of recursion on a child is when it reaches two sinks. Since sinks
are stored in parents, the recursive call inserted into \ApplyQrec\ can be
omitted in favour of applying the operator on both sinks. Hence, given the
label, low and high nodes from Code~\ref{lst:apply_node} and the operand, then
the node to be outputted can be resolved as depicted in
\lstinline{apply_recurse_node} in Code~\ref{lst:apply_recurse_node}.

\begin{lstfloat}[ht!]
  \centering

  \input{../listing/apply_recurse_node.tex}

  \caption{Construction of to-be-outputted node \lstinline{v'} and recursing
    using \ApplyQrec.}
  \label{lst:apply_recurse_node}
\end{lstfloat}

Putting everything above together, we can create the \Apply\ algorithm as
presented in Code~\ref{lst:apply_algorithm}

\begin{lstfloat}[ht!]
  \centering

  \input{../listing/apply_algorithm}

  \caption{The \Apply\ algorithm}
  \label{lst:apply_algorithm}
\end{lstfloat}

\clearpage
Let $N_1$ and $N_2$ be the respective sizes of $G_1$ and $G_2$. By the nature of
how both lists are scanned, then only $O(N_1 + N_2)$ I/Os are spent on scanning
the nodes. There is at most $N_1 \cdot N_2$ many nodes being outputted,
resulting in at most $O(N_1 N_2)$ I/Os spent on writing the output for the later
\Reduce\ that will take $O(\sort(N_1 N_2))$ I/Os. Each element creates up to two
requests for recursions, each of which may be reinserted to forward data later
into the queue. That means, an $O(N_1 N_2)$ number of elements are inserted and
retrieved from \ApplyQrec\ which requires $O(\sort(N_1 N_2))$ I/Os. By the same
account the algorithm also spends $O(N_1 N_2 \cdot \log (N_1 N_2))$ time for
computation. \cite{Arge96}

\subsection{Equality} \label{sec:theory__equal}
Two reduced OBDDs represent the same boolean function if and only if they are
isomorphic \cite[Theorem 1]{Bryant86}. This immediately yields a simple
recursive algorithm that decides the equality of two OBDDs $G_1$ and $G_2$.
Using the same approach as for the \Apply\ algorithm above and a single priority
queue \EqualQrec\, one can directly translates this algorithm into an I/O
efficient algorithm for \Equal\ that only uses $O(\sort(\min(N_1, N_2)))$ I/Os and
time.

Since nodes directly contain the label and index of their children, the
algorithm can be further optimised. Rather than testing for equality of the
label in the node pair $(v_1,v_2)$ itself, one instead checks both
$(v_1.\mathit{low}, v_2.\mathit{low})$ and $(v_1.\mathit{high},
v_2.\mathit{high})$. This makes algorithm able to terminate earlier when given
two not equal OBDDs. Since the algorithms terminates on the first violation, the
space and time usage reduces to $O(\sort(\min(N_1, N_2)))$ I/O and time
complexity.

\subsection{Relational Product} \label{sec:theory__relational_product}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
