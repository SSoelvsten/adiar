% ---------------------------------------------------------------------------- %
% OPTIMISATIONS DESCRIPTION
% ---------------------------------------------------------------------------- %
\section{Optimisations} \label{sec:optimisations}
The primary intent of the presentation of the algorithms above was to convey the
idea of Time Forward Processing applied on the topological ordering og OBDDs.
This closely followed the algorithm as in \cite{Arge96}. We will now point out
multiple avenues to improve the constants in the I/O and time complexity at the
cost of the code complexity.

\subsection{Reduce: Early application of the first reduction rule}
A node that is to be reduced by the first reduction rule is never to be
outputted, but only has to be inserted into \ReduceQdep. At the time of merging
with processed children in lines $19$ through $24$ in
Code~\ref{lst:reduce_algorithm}, the tuple \lstinline{(v,t1,a)} can immediately
be placed in \ReduceQdep\ instead of appending to $L_j$. While this requires a
second scan of \ReduceLdep\ costing linear I/Os and computation-time, this
minimises the size of $L_j$ whos elements are subsequently sorted thrice.

For $N_{\mathit{red}:1} \leq N$ many nodes that are removed by the first
reduction rule, this saves multiple $O(sort(N_{\mathit{red:1}}))$ I/Os at the
cost of a single $O(N/B)$ I/Os of scanning. Let $c_{\mathit{scan}}$ and
$c_{\mathit{sort}}$ be the respective constants, then a speedup is gained when
the following inequality is satisfied.
\begin{alignat*}{2}
  &&c_{\mathit{scan}} \cdot \frac{N}{B}
  & \leq 3 \cdot c_{\mathit{sort}} \cdot \frac{N_{\mathit{red:1}}}{B}
           \log_{M/B} \left( \frac{N_{\mathit{red:1}}}{B} \right)
  \\ \ArrowBetweenLines
  &&\frac{c_{\mathit{scan}}}{3 \cdot c_{\mathit{sort}}}
  & \leq \frac{N_{\mathit{red:1}} \log_{M/B} N_{\mathit{red:1}} / B}{N}
\end{alignat*}
Dropping the $\log_{M/B} N_{\mathit{red:1}} / B$ term gives a lower bound on the
ratio $N_{\mathit{red:1}} / N$ based on the ratio $c_{\mathit{scan}} /
c_{\mathit{sort}}$.
\begin{equation}
    \frac{c_{\mathit{scan}}}{3 \cdot c_{\mathit{sort}}} \leq \frac{N_s}{N}
\end{equation}
This constitutes a very pessimistic lower bound, as not only the term was
dropped but also it lacks accounting for the $O(N_{\mathit{red:1}} / B)$ I/Os
spent on scanning the $N_{\mathit{red:1}}$ elements in $L_j,
L_{j,\mathit{red:1}}$ and the same number of I/Os spent on moving of data.

\todo[inline]{Experimentally settle a value for the ratios $c_{\mathit{scan}} /
  c_{\mathit{sort}}$ and $N_{\mathit{red:1}} / N$ to determine whether this is
  satisfied in practice. If not, then the analysis has to be more fine-grained
  or we just have to do actual experiments on the implementation.}

\subsection{Improving memory footprint of data structures}
One should notice in all the algorithms above, that at no point is it important
to store an entire node in the transposed graph or in the priority queue. The
only information needed to be forwarded and needed to sort by is the
\lstinline{index} and \lstinline{label}. The only exception to this is the
forwarding of information across the layer in the \lstinline{data} value in the
\Apply, \Equal algorithms. In all other cases, the \lstinline{Node}s of the data
structures can be treated as the $1/3$ as big \lstinline{NodeArc.Link}. While
this does not directly change the number of entries, it does allow for a
considerable higher number of entries to be in memory at the same time.

\subsection{Improving space efficiency of Apply}
The \Apply\ algorithm as depicted in section~\ref{sec:theory__apply} is
completely decoupled from the following \Reduce. As depicted in
Figure~\ref{fig:apply_reduce_simple_pipeline} the \Apply algorithm outputs the
combined OBDD in unreduced form in \ReduceLwork\ together with the transposed
graph \ReduceLdep. This results in the \Reduce\ algorithm must work on two
inputs of quadratic size.

\begin{figure}[ht!]
  \centering

  \input{../tikz/apply_reduce_simple_pipeline.tex}
  
  \caption{The current pipeline between \Apply\ and \Reduce}
  \label{fig:apply_reduce_simple_pipeline}
\end{figure}

As such, the only part that can only be done during the top-down traversal of
the \Apply\ is the construction of the transposed graph. All other parts of the
\Apply\ algorithm can be moved into the following bottom-up traversal of the
\Reduce\ algorithm. The following \Reduce\ can even work on the original two
graphs $G_1$ and $G_2$ as the work list \ReduceLwork\ by using a similar
subprocedure like \lstinline{apply_step} and extending the priority queue
\ReduceQdep\ to forward \lstinline{data} across the layer. Given the operator,
the sink-values can also be resolved in the \Reduce.

These observations turn the pipeline of
Figure~\ref{fig:apply_reduce_simple_pipeline} to the more specialised pipeline
of Figure~\ref{fig:apply_reduce_optimised_pipeline}. This potentially reduces
the size of the work-list dramatically at the cost of a minor size-increase in
the transposed graph to contain \lstinline{Node}-tuples in both directions, and
the need to forward information across a layer during the merge with elements
from the priority queue \ReduceQdep.

\begin{figure}[ht!]
  \centering

  \input{../tikz/apply_reduce_optimised_pipeline.tex}
  
  \caption{A more space efficient pipeline between \Apply\ and \Reduce}
  \label{fig:apply_reduce_optimised_pipeline}
\end{figure}



\subsection{Representation of nodes for faster sorting}
Inspired by \cite{Dijk16} we propose to represent the \lstinline{NodeArc} type
of Code~\ref{lst:data} as a single 64-bit integer as shown in
Figure~\ref{fig:data}. A \lstinline{NodeArc.Sink} of value $v \in \{0,1\}$ is
represented by a $1$-flag on the most significant bit, $v$ on the least
significant bit, and all other bits set to $0$.\footnote{Notice, that we as such
  leave room for sinks taking on non-boolean values of up to $2^{63}$ bits.} A
\lstinline{NodeArc.Link} has a $0$-flag on the most significant bit, the next
$k$-bits dedicated to the \lstinline{label}, and finally the $63-k$ least
significant bits contain the \lstinline{value}. A reasonable value for $k$ is
$21$, since this supports an OBDD of up to $2^k = 2^{21} \approx 2 \cdot 10^7$
variables and of size up to $2^{63-k} = 2^{42} \approx 4.45 \cdot 10^{12}$
nodes.

\todo[inline]{Intermediate results may be larger than $10^{12}$ nodes. This can
  stay within this setup by resetting the ``index'' for each layer. This
  preserves the ordering.}

\begin{figure}[ht!]
  \centering

  \begin{subfigure}{0.49\linewidth}
    \centering \input{../tikz/data_sink.tex}
    \caption{\lstinline{NodeArc.Sink\{value: v\}} where \lstinline{v} $\in
      \{0,1\}$}
    \label{fig:data_sink}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering \input{../tikz/data_link.tex}
    \caption{\lstinline{NodeArc.Link\{label, index\}}}
    \label{fig:data_link}
  \end{subfigure}

  \caption{Visual representation of data layout. The least significant bit is
    right-most.}
  \label{fig:data}
\end{figure}

A \lstinline{Node} can then be represented by $3$ $64$-bit numbers: Two $64$-bit
integers for each child and another $64$ bits for the \lstinline{label} and
\lstinline{index} of the node itself with the same layout as for
\lstinline{NodeArc.Link}.

One should notice, that the sorting in the previous section with this layout is
a trivial sorting on $64$-bit numbers. In the case of the bottom-up sweep
algorithm, the sorting is in descending order, while the top-down sweep
algorithms use a sorting in ascending order. Tuples $(t_1,t_2)$ in \Apply\ and
\Equal\ are then $64$-bit numbers $\min(t_1,t_2)$ when \lstinline{data} is not
present. Since \lstinline{data} is only present for pairs of nodes with the same
\lstinline{label}, then the number to sort by simply becomes $\max(t_1,t_2)$.
Whether it is faster to repeatedly compute $\min$ and $\max$ during sorting, or
precompute it at insertion time at the cost of increasing the size of each
element by another $64$-bits, is worth experimental investigation.

The representation of variables \lstinline{Node} and \lstinline{NodeArc} as a
single $64$-bit integers does result in having to do bit shifts and bit masking
to access the values. Such instructions though are computationally very cheap,
and are quickly offset by the smaller memory size and especially the much
cheaper comparisons during sorting. This reduction to numbers might even open up
investigating the use of $O(N)$ time and $O(\sort(N))$ I/O non-comparative sorting
algorithm, such as the ones in \todocite.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
