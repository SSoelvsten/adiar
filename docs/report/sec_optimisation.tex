% ---------------------------------------------------------------------------- %
% OPTIMISATIONS DESCRIPTION
% ---------------------------------------------------------------------------- %
\section{Optimisations} \label{sec:optimisations}
The primary intent of the presentation of the algorithms above was to convey the
idea of Time Forward Processing applied on the topological ordering og OBDDs. We
will now point out multiple avenues to improve the constants in the I/O and time
complexity at the cost of the code complexity.

\subsection{Reduce}
Since \Reduce\ is used at the end of all other algorithms, any small ounce of
speed one can squeeze out of this function will have noticable implications on
the speed of the others. In the presentation of the algorithm above, we already
improved the algorithm by merging the input OBDD with the supporting data
structure of the transposed graph, but we also propose the optimisations below.

\subsubsection{Separate sink-dependency list}
The main bottleneck of the \Reduce\ algorithm is the use of the priority queue
\ReduceQdep, why we would want to minimise the number of elements within. In the
algorithm we assumed \ReduceQdep\ is prepopulated with the sink dependencies and
were able to do do so in all prior algorithms.

We notice, that the sink-dependencies always are placed in \ReduceQdep\ in
topological order, which means all \lstinline{Edges} are sorted with respect to
their \lstinline{source}. Hence, one can instead prepopulate a list
$L_{\mathit{Red:}Sinks}$, from which the sink dependencies are retrieved rather
than from \ReduceQdep.

\todo[inline, caption={Experiment: ratio of sink-arcs}]
{Experimentally settle the ratio of sinks relative to $N$ to discern the
  possible speedup.}

\subsubsection{Early application of the first reduction rule}
At the time of extracting the dependencies \lstinline{e_low} and
\lstinline{e_high} of a node \lstinline{v} from \ReduceQdep, one already has all
the information needed to potentially apply the first reduction rule. Hence, one
may immediately populate the list of rule 1 reduced nodes,
$L_{j:\mathit{red}:1}$ instead of extracting it later in
\lstinline{reduce_layer}. This will save one from having to scan and sort those
elements multiple times.

\todo[inline, caption={Experiment: ratio of rule-1 reduced}]
{Experimentally settle a value the number of rule-1 reduced nodes.}

\subsubsection{Implicit merge of the reduction lists}
One should notice, that one does not need to concatenate and sort the three
lists $L_{j,\mathit{out}}$, $L_{j,\mathit{red:}1}$, $L_{j,\mathit{red:}2}$ to
create $L_{j:F}$, since one may instead merely sort the three lists and
implicitly take \lstinline{(w,w')} from $L_{j:F}$ by picking the next element
from either of the three lists, similarly to the classic \lstinline{merge}
subprocedure of merge sort.

\subsection{Apply}
Some boolean operators, such as \emph{or}, \emph{and}, and \emph{implication}
all allow for the ability to short-circuit their evaluation. At the start of the
\Apply\ one can evaluate all combinations of the given operator \lstinline{op}
to then conclude what evaluations can be short-circuited. This may decrease the
number of requests in \ApplyQrec\ when sink-nodes become part of the requests.

\todo[inline, caption={Experiment: Efficiency of short-circuit}]
{ Experimentally conclude how many requests and intermediate nodes one can save
  when choosing to short-circuit. }

\subsection{Exists}
\todo[inline, caption={Algorithm: Reduce optimisations that apply to Exists}]
{ There are quite a few optimisations from Reduce that apply, remember to list
  them. }

\todo[inline, caption={Algorithm: Process multiple layers in Exists}]
{ As long as there are no edges between nodes of a layer, then they are
  independent and one can even process all these layers in bulk. }

\subsection{Omitting the index} \label{sec:optimisations__no_index}
When one processes a large data set, then one inevitably will do so by
processing a stream. This will then be with an iterator with which one may
\emph{seek} forward or backwards through the stream. Due to the physical nature
of harddisks, almost no time is lost due to this; the readhead still needs to
move, the disk needs to spin, and the branch prediction is able to prefetch a
lot of values.

Since all \lstinline{Nodes} and \lstinline{Edges} are sorted with respect to a
\lstinline{label} and \lstinline{id}, then the index itself is not needed to
identify the needed value. Hence, to save space, we may choose to completely
omit the index and instead only rely on the ordering.

\subsection{Representation of nodes for faster sorting}
Inspired by \cite{Dijk16} we propose to represent the \lstinline{NodeArc} type
of Code~\ref{lst:data_node} as a single 64-bit integer as shown in
Figure~\ref{fig:data}. A \lstinline{NodeArc.Sink} of value $v \in \{0,1\}$ is
represented by a $1$-flag on the most significant bit, $v$ on the least
significant bit, and all other bits set to $0$.\footnote{Notice, that we as such
  leave room for sinks taking on non-boolean values of up to $2^{63}$ bits.} A
\lstinline{NodeArc.Ptr} has a $0$-flag on the most significant bit, the next
$k$-bits dedicated to the \lstinline{label}, and finally the $63-k$ least
significant bits contain the \lstinline{id}.

A \lstinline{Node} can then be represented by $3$ $64$-bit numbers:
Two $64$-bit integers for each child and another $64$ bits for the
\lstinline{label} and \lstinline{index} of the node itself with the same layout
as for \lstinline{NodeArc.Ptr}.

\begin{figure}[ht!]
  \centering

  \begin{subfigure}{0.49\linewidth}
    \centering \input{../tikz/data_sink.tex}
    \caption{\lstinline{NodeArc.Sink\{value: v\}} where \lstinline{v} $\in
      \{0,1\}$}
    \label{fig:data_sink}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering \input{../tikz/data_link.tex}
    \caption{\lstinline{NodeArc.Ptr\{label, index\}}}
    \label{fig:data_link}
  \end{subfigure}

  \caption{Visual representation of data layout. The least significant bit is
    right-most.}
  \label{fig:data}
\end{figure}

Reexamining the algorithms above, one will notice that only the combination of
\lstinline{label} and \lstinline{id} has to be unique, not \lstinline{id} by
itself. Hence, \lstinline{id} may be reset for each layer, thereby enlarging the
possible size of each layer and with it the total number of nodes in the graph.
A reasonable value for $k$ is $28$, since this supports OBDDs of up to $2.6
\cdot 10^8$ variables, $3.4 \cdot 10^{10}$ nodes per layer for a total
$2^{63}$ nodes in the OBDD.

One should notice, that the sorting in the previous section with this layout in
a single number is now reduced to a mere trivial sorting on $64$-bit numbers. In
the case of the bottom-up sweep algorithm, the sorting is in descending order,
while the top-down sweep algorithms is in ascending order. Tuples $(t_1,t_2)$ in
\Apply\ and \Isomorphic\ are then $64$-bit numbers $\min(t_1,t_2)$ when
\lstinline{data} is not present. Since \lstinline{data} is only present for
pairs of nodes with the same \lstinline{label}, then the number to sort by
simply becomes $\max(t_1,t_2)$. Whether it is faster to repeatedly compute
$\min$ and $\max$ during sorting, or precompute it at insertion time at the cost
of increasing the size of each element by another $64$-bits, is worth
experimental investigation.

\todo[inline, caption={Experiment: Precompute sorting number}]
{Experimentally settle whether one should precompute for the sorting algorithm
  or instead save space usage.}

The representation of variables \lstinline{Node} and \lstinline{NodeArc} as a
single $64$-bit integers does result in having to do bit shifts and bit masking
to access the values. Such instructions though are computationally very cheap,
and are quickly offset by the smaller memory size and especially the much
cheaper comparisons during sorting. This reduction to numbers might even open up
investigating the use of $O(N)$ time and $O(\sort(N))$ I/O non-comparative sorting
algorithm, such as the ones in \todocite.

\subsection{Equality checking}
One should notice, that all algorithms as they have been described above are
\emph{stable} in the sense, that the ordering of nodes within each layer is
preserved. That means, one can check whether an OBDD $G$ is unchanged after one
or more manipulations by comparing it with the result $G'$ in a single linear
scan, which only takes $O(N/B)$ I/Os.

\todo[inline, caption={Algorithm: $O(1)$ time change checking}]
{This is still too slow! In model checking, one wants to compute the transitive
  closure by repeatedly applying \RelProd\ and checking whether the output is
  unchanged. This check is in conventional OBDD libraries done in $O(1)$ time
  with a mere pointer comparison.
  \begin{center}
    We should be able to do the same with taint tracking?
  \end{center}
}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% ispell-local-dictionary: "british"
%%% End:
