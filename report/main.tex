% ---------------------------------------------------------------------------- %
% INTRODUCTION
% ---------------------------------------------------------------------------- %
\section{Introduction} \label{sec:intro}

% ---------------------------------------------------------------------------- %
% PRELIMINARIES
% ---------------------------------------------------------------------------- %
\section{Preliminaries} \label{sec:preliminaries}

\subsection{The I/O Model}

\subsection{Ordered Boolean Decision Diagrams}

\begin{theorem}[\cite{Arge96}] \label{thm:reduce_io_lower_bound}

  Reduction of an OBBD $G$ with minimal pair, level, depth first or breadth
  first blocking requires $\Omega(\sort(N))$ I/Os in the worst case.
\end{theorem}

\begin{theorem}[\cite{Arge96}] \label{thm:apply_io_worst_case}

  The Dynamic Programming \Apply\ algorithm on two OBDDs of size $N_1, N_2$
  followed up by a \Reduce\ operation requires $O(N_1 \cdot N_2)$ I/Os in the
  worst case.
\end{theorem}


% ---------------------------------------------------------------------------- %
% THE ALGORITHM IN THEORY
% ---------------------------------------------------------------------------- %
\newpage
\section{Cache-oblivious OBBD algorithms} \label{sec:theory}

For completeness we provide both the \Reduce\ and \Apply\ algorithms of
\cite{Arge96}. We expand on these algorithms with a set of other algorithms for
the manipulation of OBBD.

The underlying idea of all the algorithms given below is to exploit that OBBD's
are directed acyclic graphs. If all nodes of an OBBD is sorted first by their
label and secondly by their unique identifier, then that constitutes a
topological sorting. As long as the dependency of computation only is one-way in
the OBBD, then a recursive algorithm can instead be handled by scanning through
all nodes in topological order and ``forward'' information to its children or
parent. This \emph{Time-Forward Processing} is done by use of one more priority
queues that are carefully synchronised with the scanning of the nodes \todocite.

These queues are aligned with the scan through by sorting all its entries by the
same ordering as the nodes. At the time of forwarding the information, the
receiving node is most likely not in memory. Hence, a lookup of the receiving
node would result in a full I/O used, which is not desired. To mitigate this,
all nodes store both the label and unique identifier of their children, as shown
in Code~\ref{lst:struct}.
\begin{lstfloat}[ht!]
  \centering

  \begin{blstlisting}[language=pseudo, numbers=none]
  Type NodeArc = Sink of { value: bool }
               | Link of { index: int, label: int }

  Struct Node {
    index : int;
    label : int;
    low   : NodeArc;
    high  : NodeArc;
  }
  \end{blstlisting}

  \caption{The information stored in each node of the OBBD}
  \label{lst:struct}
\end{lstfloat}

Since the label for the children is stored within the parent, then there is no
need to explicitly store the $0$- and $1$-sinks as nodes in the graph. Further
notice, that the Node contains no direct pointers to its children, but instead
only an index of where to find it an array. This is due to the fact that the
\Reduce\ algorithm described below outputs the nodes bottom-up and hence is able
to provide the index of the prior outputted children. That means, that an OBBD
as outputted by \Reduce\ can be traversed with the \Evaluate\ function in
Code~\ref{lst:eval}, which for $n$ variables this takes $O(n)$ time and $O(n)$
I/Os.
\begin{lstfloat}[ht!]
  \centering

  \begin{blstlisting}
  Evaluate(x : bool[], G : OBDD):
    v := G.V[G.V.length - 1] // The root is outputted last by Reduce

    while True:
      if x[node.label]:
        case v.high of Sink{ value }        => return value
                     | Link{ index, label } => v := G.V[index]
      else:
        case v.low of Sink{ value }         => return value
                    | Link{ index, label }  => v := G.V[index]
  \end{blstlisting}

  \caption{The \Evaluate\ algorithm to traverse a reduced OBBD $G$ according to an
    assignment $x$}
  \label{lst:eval}
\end{lstfloat}

Except \Reduce, all algorithms given below assume that the input OBBD is reduced
and in reverse topological order, as outputted by the \Reduce\ algorithm. On the
other hand, the \Reduce\ algorithm expects the to-be-reduced list of nodes are
in topological order. Due to this, all sorting steps of the input nodes can
be omitted for the sake of optimisation. \cite{Arge96}

\subsection{Reduce} \label{sec:theory_reduce}
The \Reduce\ algorithm makes use of the following three data structures:

\begin{itemize}
\item The list \ReduceLwork\ of all nodes $v \in V$ sorted with respect to $v$.

  This is to be used as the work-list of processing the nodes for the reduction.

\item The list \ReduceLdep\ of 3-tuples
  \lstinline{(s: Node, t: NodeArc.Link, b: bool)}
  sorted with respect to the label and index of \lstinline{t}.

  This list is the transposed graph and is used to provide the node
  \lstinline{s} to which the result of processing its child \lstinline{t} has to
  be sent.

\item The priority queue \ReduceQdep\ of 3-tuples
  \lstinline{(s: Node, t': NodeArc.Link, b: bool)} sorted with
  respect to \lstinline{s} and secondly by \lstinline{b}.

  This priority queue is used to forward the result \lstinline{t'} of processing
  nodes \lstinline{t} $\in \ReduceLwork$ to nodes in $\ReduceLwork$ dependent on
  the result \lstinline{t'}.
\end{itemize}
These datastructures can be initialised as depicted in
Code~\ref{lst:reduce_init}. \ReduceLwork\ can be populated in topological
order in $O(\sort(N))$ I/Os. \ReduceLdep\ can also be populated with all
dependencies to non-sink children and sorted $O(\sort(N))$ I/Os. For simplicity
of the presentation of the \Reduce\ algorithm later we populate \ReduceQdep\
with all results of computation to the implicitly-stored sink nodes.

\begin{lstfloat}[ht!]
  \centering

  \begin{blstlisting}
  $\ReduceLwork$ := G.V : Node[]
  sort nodes in $\ReduceLwork$

  $\ReduceLdep$ := [] : (Node, NodeArc, bool)[]
  $\ReduceQdep$ := $\emptyset$ : PriorityQueue<(Node,NodeArc,bool)>

  for $v \in \ReduceLwork$:
      low_dependency := (v, v.low, low)
      high_dependency := (v, v.high, high)

      case v.low of Sink{_}   => $\ReduceQdep$.insert(low_dependency)
                  | Link{_,_} => $\ReduceLdep$.append(low_dependency)

      case v.high of Sink{_}   => $\ReduceQdep$.insert(high_dependency)
                   | Link{_,_} => $\ReduceLdep$.append(high_dependency)

  sort (s,t,a) in $\ReduceLdep$ by t
  \end{blstlisting}

  \caption{Initialisation of datastructures for \Reduce}
  \label{lst:reduce_init}
\end{lstfloat}

The \Reduce\ algorithm applies the reduction rules of \textcite{Bryant86} on all
nodes of each layer using the \lstinline{ReduceLayer} subroutine depicted in
Code~\ref{lst:reduce_bryant}. The output is are three lists
\lstinline{O}$_{\mathit{out}}$, \lstinline{O}$_{\mathit{red:}1}$,
\lstinline{O}$_{\mathit{red:}2}$ of tuples \lstinline{(w,w',i)} of mapping
between the original nodes \lstinline{w} of the input and nodes \lstinline{w'}
to-be or already outputted. The \lstinline{i} value contains the index of the
original node \lstinline{w} for reasons that will become apparent later.

\begin{lstfloat}[ht!]
  \centering

  \begin{blstlisting}
  reduce_layer(data: (Node, NodeArc, NodeArc)[]):
      sort (v, low, high) in data by low first and high second
      O$_{\mathit{out}}$, O$_{\mathit{red:}1}$, O$_{\mathit{red:}2}$ = []

      for i := 0 to data.length - 1:
          (w, w_low, w_high) = data[i]

          // Reduction rule 1:
          if w_low = w_high:
              O$_{\mathit{red:}1}$.append((w,w_low,w.index))
          else:
              w' = {
                index: NIL,
                label: w.label,
                low: w_low,
                high: w_high
              }

              O$_{\mathit{out}}$.append(w,w',w.index)

              // Reduction rule 2:
              while i < data.length:
                  (o, o_low, o_high) = data[i]
                  if w_low = o_low $\land$ w_high = o_high:
                      O$_{\mathit{red:}2}$.append((o,w',w.index))
                      i++

      return (O$_{\mathit{out}}$, O$_{\mathit{red:}1}$, O$_{\mathit{red:}2}$)
  \end{blstlisting}

  \caption{Subroutine applying reduction rules of \cite{Bryant86} within \Reduce}
  \label{lst:reduce_bryant}
\end{lstfloat}

We are now ready to present the \Reduce\ algorithm. The algorithm takes as
argument all the three datastructures described above and assumes they all are
initialised. We choose to do this, to make explicit the ability of the later
algorithms to populate all three datastructures for free. The \Reduce\ algorithm
is presented in Code~\ref{lst:reduce_algorithm}.

It proceeds by computing and outputting the reduced OBBD bottom-up and layer for
layer. This is done by a single scan through the work list \ReduceLwork, where
the nodes of each layer are merged with the result of the recursion to then
apply the reduction rules of \cite{Bryant86} on the whole layer. The recursive
dependencies are handled through \ReduceQdep\ which is synchronised with the
scan of \ReduceLwork\ while using \ReduceLdep\ to forward in \ReduceQdep\ the
outputted result to dependent nodes.

\begin{lstfloat}
  \centering

  \begin{blstlisting}
    Reduce($\ReduceLwork$: Node[],
           $\ReduceLdep$: (Node,NodeArc,bool)[],
           $\ReduceQdep$: PriorityQueue<(Node$\times$NodeArc$\times$bool)>):
      // Index of next output in stream
      iO = 0

      // Index into *@\color{cGray} $\ReduceLwork$@*
      iW := $\ReduceLwork$.length - 1
      v := $\ReduceLwork$[iW]

      // Index into *@\color{cGray} $\ReduceLdep$@*
      iD = $\ReduceLdep$.length - 1

      // Process bottom-up each layer except for the root-layer
      for j := n-1 downto 2:
          $L_{j}$ := []

          // Merge nodes of layer j with result of prior computations
          while v.label = j:
              (_, t1, _) := $\ReduceQdep$.extract_max()
              (_, t2, _) := $\ReduceQdep$.extract_max()

              $L_{j}$.append((v,t1,t2))
              v := $\ReduceLwork$[--iW]

          // Apply reduction rules
          ($L_{j,\mathit{out}}$, $L_{j,\mathit{red:}1}$, $L_{j,\mathit{red:}2}$) = reduce_layer($L_{j}$)

          // Output to-be outputted nodes and distribute output index
          sort (w,w',i) in $L_{j,\mathit{out}}$ and $L_{j,\mathit{red:}2}$ by i

          iR2 = 0
          for (w,w',i) in $L_{j,\mathit{out}}$:
              w'.index := iO++
              output w'

              while iR2 < $L_{j,\mathit{red:}2}$.length $\land$ $L_{j,\mathit{red:}2}$[iR2] is (_,_,i):
                  $L_{j,\mathit{red:}2}$[iR2++][1].index := w'.index

          // Forward processing information to layers j-1 .. 1
          $L_{j,\mathit{forward}}$ := $L_{j,\mathit{out}}$ ++ $L_{j,\mathit{red:}1}$ ++ $L_{j,\mathit{red:}2}$
          sort (w,w',i) in $L_{j,\mathit{forward}}$ by w.index

          (s, t, b) := $\ReduceLdep$[iD]
          for (w,w') in $L_{j,\mathit{out}}$, $L_{j,\mathit{red:}2}$:
              while t = w:
                  $\ReduceQdep$.insert((s,w',b))
                  (s, t, b) := L2[--iD]

      // Process the root
      (_, t_low,  _) := $\ReduceQdep$.extract_max()
      (_, t_high, _) := $\ReduceQdep$.extract_max()
      output { label: v.label, index: iO, low: t_low, high: t_high }
  \end{blstlisting}

  \caption{The \Reduce\ algorithm}
  \label{lst:reduce_algorithm}
\end{lstfloat}

As mentioned, \ReduceLwork\ and \ReduceLdep\ can be initialised in $O(sort(N))$
I/Os. For each element of \ReduceLdep\ a single element will be added to
\ReduceQdep, which will result in a total $4 N$ insertions and deletions from
\ReduceQdep, which also happens in worst-case $O(sort(N))$ I/Os when using an
I/O efficient priority queue. On each layer all nodes are sorted thrice, which
when all layers are combined results in another $O(sort(N))$ total number of
I/Os. Hence, the algorithm runs within the optimal $O(sort(N))$ I/O bound.
\cite{Arge96} By similar obsevations we can also conclude, that the algorithm
has an $O(N \log N)$ time complexity.

\subsubsection{Optimisations} \label{sec:reduce_optimisations}
The primary intent of the presentation above of the \Reduce\ algorithm has been
to convey the idea of Time Forward Processing applied on the topological
ordering og OBDDs. We now list multiple possible ways to improve the constants
in the I/O and time complexity at the cost of the code complexity.

\begin{itemize}
\item The merging of a node $v \in \ReduceLwork$ with the processed children
  retrieved from the queue \ReduceQdep\ serves to connect the resulting $v'$ to
  its already outputted children. A child of $v$ that is a sink is already fully
  reduced. A sink neither is an outputted element, since sinks always are stored
  directly their parents. Hence, the information stored in $v$ is sufficient
  for its sink children.

  Then, \ReduceQdep\ can be initialised to $\emptyset$ at the cost of a few
  conditional statements on $v$ before retrieving $0$, $1$, or $2$ elements out
  of \ReduceQdep\ on lines $20$ through $21$ in Code~\ref{lst:reduce_algorithm}

\item A node that is to be reduced by the first reduction rule is never to be
  outputted, but only has to be inserted into \ReduceQdep. At the time of
  merging with processed children in lines $19$ through $24$ in
  Code~\ref{lst:reduce_algorithm}, the tuple \lstinline{(v,t1,a)} can be
  forwarded in \ReduceQdep\ instead of appending to $L_j$. While this requires
  an early simultaneous scan of \ReduceLdep\ costing a linear number of I/Os
  and computation-time, this minimises the size of $L_j$ whos elements are
  subsequently sorted thrice.
\end{itemize}
While it is possible in the later algorithms to populate \ReduceQdep\ with the
data as in Code~\ref{lst:reduce_init} we will for the sake of clarity instead in
the subsequent sections assume that \ReduceQdep\ should be empty. By the
argument above this will only result in a speedup of both algorithms.

\subsection{Substitute} \label{sec:theory_substitute}
Before we present the \Apply\ algorithm we present the simpler top-down
algorithm \Substitute, which given an OBDD $G$ and an assignment vector $A$ of
tuples $(x_i, \mathit{value})$ outputs a reduced OBDD $G_{| x_i = v_i ,\ \forall
  (x_i,v_i) \in A}$ as shown in Code~\ref{lst:substitute}.  This uses the data
structure:

\begin{itemize}
\item The priority queue \SubstituteQrec\ of 3-tuples
    \lstinline{(s: Node, t: NodeArc.Link, b: bool)} sorted with respect to
    \lstinline{t} only.

  These $(s,t,b)$-tuples in the priority queue are used to forward recursion
  requests from $s$ to non-sink children $t$ along a $b$-arc.
\end{itemize}

The algorithm as depicted in Code~\ref{lst:substitute} essentially traverses the
OBDD recursively, either keeping the node and recursing along both outgoing arcs
or skipping the node and only recursing on the arc according to the assignment.
The recursion is controlled using \SubstituteQrec. Since this only traverses the
data structure top-down, the nodes traversed do not know the index of their
children before they are outputted later. Hence, these nodes will not contain an
actual index to their children. Luckily, these references can be fixed as part
of the bottom-up traversal of the final \Reduce. Furthermore, due to the
top-down traversal of the OBDD, the \ReduceLwork\, \ReduceLdep\ of \Reduce\ can
be initialised during the traversal.

\begin{lstfloat}
  \centering

  \begin{blstlisting}
  Substitute(G : OBDD, A : (int, bool)[]):
      sort (l,v) in A *@by@* l
      $\SubstituteQrec$ = $\emptyset$ : PriorityQueue<(Node$\times$NodeArc.Link$\times$bool)>
      $\ReduceLwork, \ReduceLdep$ := [] : Node[] | (Node,NodeArc,bool)[]

      iA := 0
      (a_label, a_value) := A[iA]

      // Process the root and create initial recursion requests
      v = G.V[G.V.length - 1]
      if a_label = v.label:
          if a_value:
              if is_sink(v.high): return sink-only OBDD v.high.value
              $\SubstituteQrec$.insert ((NIL, v.high, True))
          else:
              if is_sink(v.low): return sink-only OBDD v.low.value
              $\SubstituteQrec$.insert((NIL, v.low, False))

          (a_label, a_value) := A[++iA]
      else:
          $\ReduceLwork$.append(v)
          high_request := (v, v.high, True)
          low_request := $\SubstituteQrec$.insert(v, v.low, False)

          if !is_sink(v.high): $\SubstituteQrec$.insert(high_request)
          if !is_sink(v.low):  $\SubstituteQrec$.insert(low_request)

      // Process all to-be-visited nodes in topological order
      while !$\SubstituteQrec$.empty():
          (s,t,a) := $\SubstituteQrec$.extract_min()
          v := G.V[t.index]

          // Skip to-be-substituted variables that we didn't see
          while a_label < v.label: (a_label, a_value) := A[++iA]

          // Forward the desired information
          if a_label <> v.label:
              $\ReduceLwork$.append(v)
              $\SubstituteQrec$.insert((v, v.low, False), (v, v.high, True))

              // v is outputted: dependencies (if any) to v can be
              // locked in place
              if s <> NIL:
                  while True:
                      $\ReduceLdep$.append((s,t,a))
                      if $\SubstituteQrec$.peek_min()[1] = v:
                          (s,t,a) := $\SubstituteQrec$.extract_min()
                      else: break
          else:
              rec_child := if a_value then v.high else v.low
              recurse := !is_sink(rec_child)
              request := (s, rec_child, a_value)

              if recurse: $\SubstituteQrec$.insert(request)
              else if s = NIL: return sink-only OBDD rec_child.value

      return Reduce($\ReduceLwork, \ReduceLdep, \emptyset$)
  \end{blstlisting}

  \caption{The \Substitute\ algorithm}
  \label{lst:substitute}
\end{lstfloat}

By the ordering of tuples in \SubstituteQrec, nodes are visited in topological
order, which also is the blocking of on disc \GV. This means every block is
fetched at most once and at most $O(N/B)$ I/Os are used as part of the look up
of vertices in \GV. At most $2N$ requests are placed and retrieved from \Q\
which will only require $O(sort(N))$ I/Os. let $N' \leq N$ be the number of
nodes outputted for the later \Reduce, which then also only uses another
$O(sort(N')) = O(sort(N))$ I/Os. In total the \Substitute\ algorithm performs
the full substitution and reduction in $O(sort(N))$ I/Os and $O(N \log N)$ time.

\subsection{Apply}
Similar to \Substitute, the \Apply\ algorithm makes use of a priority queue
to forward the recursion to nodes visited later in topological order. Since the
queue has to be synchronised with the scan of both list of nodes at the same
time, then the sorting is a bit more complicated than in the prior given
algorithms. The data structure used is:
\begin{itemize}
\item The priority queue \ApplyQrec\ of 4-tuples \lstinline{((s$_1$,s$_2$), (t$_1$,t$_2$), b, data)}
  where \lstinline{s$_1$, s$_2$} are of type \lstinline{Node}, \lstinline{t$_1$, t$_2$}
  of type \lstinline{NodeArc}, \lstinline{b} of \lstinline{bool} and
  \lstinline{data} of \lstinline{Option(Node)}.

  This forwards requests for recursion from processing the pair of nodes $(s_1,
  s_2)$ to $(t_1,t_2)$ through a $b$-arc. The \lstinline{data} element is used
  when $t_1$ and $t_2$ have the same label such that the first visited can be
  forwarded to to the second for processing. Hence, the sorting of an element is
  dependant on whether \lstinline{data} is provided in the tuple.

  \begin{itemize}
  \item If \lstinline{data} is \lstinline{None}, then it is sorted first with
    respect to $\min(t_1.\mathit{label},t_2.\mathit{label})$, secondly with
    respect to $\min(t_1.\mathit{index},t_2.\mathit{index})$. Sinks come after
    nodes.

  \item If \lstinline{data} is \lstinline{Some(v)}, then it is still first
    sorted with respect to $\min(t_1.\mathit{label},t_2.\mathit{label})$, but
    secondly dually with respect to $\max(t_1.\mathit{index},
    t_2.\mathit{index})$. For two requests to the same $(t_1,t_2)$ the one with
    \lstinline{data} comes first.
  \end{itemize}
\end{itemize}
The reader should notice that in the general case of \lstinline{data} being
\lstinline{None}, then the sorting is a generalisation of the sorting of nodes
used up until this point.

To match the general sorting above the \lstinline{apply_step} function below in
Code~\ref{lst:apply_step} mutates variables $\mathit{iW}_1$ and $\mathit{iW}_2$
being indices in the respective input lists $G_1.V$ and $G_2.V$ of vertices
together with the matching nodes $v_1$ and $v_2$. This function used later in
\Apply, where the mutated variables are initialised.
\begin{lstfloat}
  \centering

  \begin{blstlisting}
  apply_step():
      if v$_1$.label <> v$_2$.label:
          if v$_1$.label < v$_2$.label:
              v$_1$ := G$_1$.V[++iW$_1$]
          else:
              v$_2$ := G$_2$.V[++iW$_2$]
      else:
          if v$_1$.index > v$_2$.index:
              v$_2$ := G$_2$.V[++iW$_2$]
          else:
              v$_1$ := G$_1$.V[++iW$_1$]
  \end{blstlisting}

  \caption{Determining in which OBDD should be progressed matching the sorting}
  \label{lst:apply_step}
\end{lstfloat}

The \Apply\ algorithm constructs an OBDD that simulates both OBBDs $G_1$ and
$G_2$ at the same time. At a node representing a $(v_1,v_2)$ the label of the
output node is $\min(v_1.\mathit{label},v_2.\mathit{label})$. In the case the
two labels of $v_1$ and $v_2$ differ, then the recursion is only dependent on
the node with the smallest label. Otherwise, \lstinline{data} is used to send
the data of the first-seen node to the second for processing later, when both
are in memory. That is, for all cases but when \lstinline{data} should be
forwarded then whereto recurse can be resolved as depicted in
Code~\ref{lst:apply_node}.

\begin{lstfloat}[ht!]
  \centering

  \begin{blstlisting}
  apply_node((v$_1$, v$_2$) : Node$\times$Node,
             (t$_1$, t$_2$) : NodeArc$\times$NodeArc,
             data : Option(Node)):
      label := min(t$_1$.label = t$_2$.label)

      if $_1$.label <> t$_2$.label:
          if label = v$_1$.label:
              low := (v$_1$.low, t$_2$)
              high := (v$_1$.high, t$_2$)
          else:
              low := (v$_1$.low, t$_2$)
              high := (v$_1$.high, t$_2$)
      else:
          v$_1'$ := case data of None => v$_1$
                         | Some(v$_o$) => if v$_o$.index = t$_1$.index
                                     then v$_o$ else v$_1$
          v$_2'$ := case data of None => v$_2$
                         | Some(v$_o$) => if v$_o$.index = t$_2$.index
                                     then v$_o$ else v$_2$

          low := ((v$_1'$.low, v$_2'$.low)
          high := ((v$_1'$.high, v$_2'$.high)

      return (label, low, high)
  \end{blstlisting}

  \caption{Resolving which direction to recurse on request for $(w_1, w_2)$ while
    being at $(v_1, v_2)$.}
  \label{lst:apply_node}
\end{lstfloat}

Similar to \Substitute, this makes the algorithm traverse both OBDDs top-down
and hence output nodes before their children. This leaves no ability to include
a direct reference to them. Again, this is fixed during the later \Reduce, for
which the sorting is satisfied by merely supplying indices in order of
outputting the vertices.

The base case of recursion on a child is when it reaches two sinks. Since sinks
are stored in parents, the recursive call inserted into \ApplyQrec\ can be
omitted in favour of applying the operator on both sinks. Hence, given the
label, low and high nodes from Code~\ref{lst:apply_node} and the operand, then
the node to be outputted can be resolved as depicted in
\lstinline{apply_recurse_node} in Code~\ref{lst:apply_recurse_node}.

\begin{lstfloat}[ht!]
  \centering

  \begin{blstlisting}
  apply_recurse_node(label: int,
                     low, high: NodeArc$\times$NodeArc,
                     op: bool$\times$bool $\rightarrow$ bool):
        low' = case low of
               | (Sink{value$_1$}, Sink{value$_2$}) => Sink{op(value$_1$, value$_2$)}
               | _ => low

        high' = case high of
                | (Sink{value$_1$}, Sink{value$_2$}) => Sink{op(value$_1$, value$_2$)}
                | _ => low

         v' := { index: iO--, label: label, low: low', high: high' }
         if !is_sink(low'): $\ApplyQrec$.insert((v,low',False,None))
         if !is_sink(high'): $\ApplyQrec$.insert((v,high',False,None))

         return v'
  \end{blstlisting}

  \caption{Construction of to-be-outputted node \lstinline{v'} and recursing
    using \ApplyQrec.}
  \label{lst:apply_recurse_node}
\end{lstfloat}

Putting everything above together, we can create the \Apply\ algorithm as
presented in Code~\ref{lst:apply_algorithm}

\begin{lstfloat}[ht!]
  \centering

  \begin{blstlisting}
  Apply(G$_1$ : OBDD, G$_2$ : OBDD, op: bool$\times$bool -> ):
     $\ReduceLwork, \ReduceLdep$ := [] : Node[] | (Node,NodeArc,bool)[]
     $\ApplyQrec$ := $\emptyset$ : PriorityQueue<...>
     iW$_1$ := G$_1$.V.length - 1
     iW$_2$ := G$_2$.V.length - 1

     v$_1$ := G$_1$.V[iW$_1$]
     v$_2$ := G$_2$.V[iW$_2$]

     // Process root and create initial recursion requests
     low = compare v$_1$.label to v$_2$.label:  <  => (v$_1$.low, v$_2$)
                                       | > => (v$_1$, v$_2$.low)
                                       | = => (v$_1$.low, v$_2$.low)

     high = compare v$_1$.label to v$_2$.label:  <  => (v$_1$.high, v$_2$)
                                        | > => (v$_1$, v$_2$.high)
                                        | = => (v$_1$.high, v$_2$.high)
     iO := 0
     v' := { index: iO, label: min(v$_1$.label, v$_2$.label), low, high }

     $\ReduceLwork$.append(v')
     $ApplyQrec$.insert((v',low,False,None), (v',high,True,None))
     iO := iO - 1

     // Process all nodes in topological order of both OBBD's
     v$_1$ := G$_1$.V[--iW$_1$]
     v$_2$ := G$_2$.V[--iW$_2$]

     while iW$_1$ < G$_1$.V.length or iW$_2$ < G$_2$.V.length:
         // Extract all next recursion request to same pair of nodes
         ((s$_1$, s$_2$), (t$_1$, t$_1$), b, data) := $\ApplyQrec$.extract_min()

         // Forward in either OBBD if none match request
         while v$_1$ <> t$_1$ $\land$ v$_2$ <> t$_2$:
            apply_step() // mutates *@\color{gray} $v_1, v_2, \mathit{iW}_1,$ and $\mathit{iW}_2$. See Code~\ref{lst:apply_step}@*

         // Forward information across the layer?
         if t$_1$.label = t$_2$.label $\land$ ((v$_1$ <> t$_1$ $\lor$ v$_2$ <> t$_2$) $\land$ data = None):
             v$_o$ := if v$_1$ = t$_1$ then v$_1$ else v$_2$
             $\ApplyQrec$.insert((s$_1$, s$_2$), (t$_1$, t$_1$), b, Some(v$_o$)))
             while $\ApplyQrec$.peek_min()[1] = (t$_1$,t$_2$):
                     ((s$_1$,s$_2$), (t$_1$,t$_1$), b, data) := $\ApplyQrec$.extract_min()
             else: break
             continue

         (label,low,high) := apply_node((v$_1$,v$_2$), (t$_1$,t$_2$))
         v' := apply_recurse_node(label,low,high)

         // Output v' and dependencies to v'
         $\ReduceLwork$.append(v')

         while True:
             $\ReduceLdep$.append(((s$_1$,s$_2$), (t$_1$,t$_1$), b))
             if $\ApplyQrec$.peek_min()[1] = (t$_1$,t$_2$):
                 ((s$_1$,s$_2$), (t$_1$,t$_1$), b, data) := $\ApplyQrec$.extract_min()
             else: break

     return Reduce($\ReduceLwork, \ReduceLdep, \emptyset$)
  \end{blstlisting}

  \caption{The \Apply\ algorithm}
  \label{lst:apply_algorithm}
\end{lstfloat}

\clearpage
Let $N_1$ and $N_2$ be the respective sizes of $G_1$ and $G_2$. By the nature of
how both lists are scanned, only $O(N_1 + N_2)$ I/Os are spent on reading the
input. There is at most $N_1 \cdot N_2$ many nodes being outputted, resulting in
at most $O(N_1 N_2)$ I/Os spent on writing the output for the later \Reduce\
that will take $O(\sort(N_1 N_2))$ I/Os. Each element creates up to two requests
for recursions. Up to half of the requests may be reinserted to forward data
later into the queue. That means, an $O(N_1 N_2)$ number of elements are
inserted and retrieved from \ApplyQrec\ which requires $O(\sort(N_1 N_2))$ I/Os.
By the same account the algorithm also spends $O(N_1 N_2 \cdot \log (N_1 N_2))$
time for computation. \cite{Arge96}

\subsection{Relational Product}


% ---------------------------------------------------------------------------- %
% THE ALGORITHM IN PRACTICE
% ---------------------------------------------------------------------------- %
\section{A Cache-oblivious OBDD implementation} \label{sec:implementation}


% ---------------------------------------------------------------------------- %
% CONCLUSION
% ---------------------------------------------------------------------------- %
\section{Conclusion} \label{sec:conclusion}


% ---------------------------------------------------------------------------- %
% References
% ---------------------------------------------------------------------------- %
\printbibliography

% ---------------------------------------------------------------------------- %
% Appendix
% ---------------------------------------------------------------------------- %
% \newpage \appendix
% \section{Appendix} \label{app:A}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "arxiv"
%%% End:
